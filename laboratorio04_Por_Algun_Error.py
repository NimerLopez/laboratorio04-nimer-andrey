# -*- coding: utf-8 -*-
"""Laboratorio04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19cc--ps-phHkDPQz2AL2WV1mWqGdUB8h

**NIMER LOPEZ,  ANDREY RODIGUES**
"""

# Cargando las librerias
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

# Cargando los datos
datos = pd.read_csv('/content/sample_data/churn.csv', delimiter=';', header=0)
print(datos.shape)

# Visualizar las primeras filas del conjunto de datos
print("Primeras 5 filas del conjunto de datos:")
print(datos.head())

# Convertir a dataframe
datos=pd.DataFrame(datos)

#Explorando la estructura del dataframe
datos.info()

# Cambiar los nombres de las columnas
new_name = {
    'churn': 'batir',
    'accountlength': 'duracion_cuenta',
    'internationalplan': 'plan_internacional',
    'voicemailplan': 'plan_correo_de_voz',
    'numbervmailmessages': 'num_mensajes_correo',
    'totaldayminutes': 'minutos_totales_dia',
    'totaldaycalls': 'llamadas_totales_dia',
    'totaldaycharge': 'cargo_total_dia',
    'totaleveminutes': 'minutos_totales_tarde',
    'totalevecalls': 'llamadas_totales_tarde',
    'totalevecharge': 'cargo_total_tarde',
    'totalnightminutes': 'minutos_totales_noche',
    'totalnightcalls': 'llamadas_totales_noche',
    'totalnightcharge': 'cargo_total_noche',
    'totalintlminutes': 'minutos_totales_internacionales',
    'totalintlcalls': 'llamadas_totales_internacionales',
    'totalintlcharge': 'cargo_total_internacional',
    'numbercustomerservicecalls': 'num_llamadas_servicio_cliente'
}

datos = datos.rename(columns=new_name)
datos.head()

# Verificar qué columnas tienen valores nulos
columnas_con_nulos = datos.columns[datos.isnull().any()]

# Mostrar las columnas con valores nulos
print("Columnas con valores nulos:")
print(columnas_con_nulos)

# Conteo de valores nulos por columna
conteo_nulos_por_columna = datos[columnas_con_nulos].isnull().sum()
print("\nConteo de valores nulos por columna:")
print(conteo_nulos_por_columna)
#No hay nulos

# Mostrar evidencia gráfica
plt.figure(figsize=(12, 6))
sns.heatmap(datos.isnull(), cbar=False, cmap='viridis')
plt.title('Mapa de Calor de Valores Nulos')
plt.show()

# Boxplot de la accountlength
sns.boxplot(data=datos['accountlength'], orient="h")

# Selecciona solo las columnas numéricas
columnas_numericas = datos.select_dtypes(include=['float64', 'int64'])

# Boxplot de todas las columnas numéricas
plt.figure(figsize=(16, 8))
sns.boxplot(data=columnas_numericas)
plt.title("Boxplot de todas las columnas numéricas")
plt.xticks(rotation=45, ha='right')
plt.show()

def imputar_valores_atipicos_con_mediana(datos):
    # Selecciona solo las columnas numéricas
    columnas_numericas = datos.select_dtypes(include=['float64', 'int64'])

    # Identifica y filtra los valores atípicos
    for columna in columnas_numericas.columns:
        # Calcula los cuartiles Q1 y Q3
        Q1 = columnas_numericas[columna].quantile(0.25)
        Q3 = columnas_numericas[columna].quantile(0.75)

        # Calcula el rango intercuartílico (IQR)
        IQR = Q3 - Q1

        # Calcula los límites superior e inferior para identificar los valores atípicos
        limite_inferior = Q1 - 1.5 * IQR
        limite_superior = Q3 + 1.5 * IQR

        # Filtra y reemplaza los valores atípicos por la mediana
        datos[columna] = np.where((datos[columna] < limite_inferior) | (datos[columna] > limite_superior),
                                  columnas_numericas[columna].median(),
                                  datos[columna])

    return datos

#imputar valores nulos con la media
def imputar_nulos_con_media(df, columna):
    media = df[columna].mean()
    df[columna].fillna(media, inplace=True)

# Identifica valores atípicos en todas las columnas numéricas
valores_atipicos = pd.DataFrame()

for columna in columnas_numericas.columns:
    # Calcula los cuartiles Q1 y Q3
    Q1 = columnas_numericas[columna].quantile(0.25)
    Q3 = columnas_numericas[columna].quantile(0.75)

    # Calcula el rango intercuartílico (IQR)
    IQR = Q3 - Q1

    # Calcula los límites superior e inferior para identificar los valores atípicos
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR

    # Filtra los valores atípicos y los agrega al DataFrame de valores atípicos
    valores_atipicos[columna] = columnas_numericas[(columnas_numericas[columna] < limite_inferior) | (columnas_numericas[columna] > limite_superior)][columna]

# Imprime los valores atípicos
print("Valores atípicos:")
# Imputar valores atípicos con la mediana
datos_imputados = imputar_valores_atipicos_con_mediana(valores_atipicos)

# Verificar que los valores atípicos hayan sido imputados
print("Valores atípicos después de la imputación:")
print(datos_imputados[(datos_imputados < limite_inferior) | (datos_imputados > limite_superior)].count())

# Selecciona solo las columnas numéricas
columnas_numericas = datos_imputados.select_dtypes(include=['float64', 'int64'])

# Boxplot de todas las columnas numéricas después de la imputación
plt.figure(figsize=(16, 8))
sns.boxplot(data=columnas_numericas)
plt.title("Boxplot de todas las columnas numéricas después de la imputación")
plt.xticks(rotation=45, ha='right')
plt.show()

"""La variable dependiente es batir, ya que es la que se desea calcular."""

datos.head()

# Convertir datos de texto a numericos
datos['plan_internacional'] = datos['plan_internacional'].map({'no': '0', 'yes':'1'})
datos['plan_correo_de_voz'] = datos['plan_correo_de_voz'].map({'no': '0', 'yes': '1'})
datos['abandono'] = datos['abandono'].map({'No': '0', 'Yes':'1'})

# Convertir datos al tipo de datos adecuado
datos = datos.astype({
    'batir': 'int64',
    'duracion_cuenta': 'int64',
    'num_mensajes_correo': 'int64',
    'minutos_totales_dia': 'float64',
    'llamadas_totales_dia': 'int64',
    'cargo_total_dia': 'float64',
    'minutos_totales_tarde': 'float64',
    'llamadas_totales_tarde': 'int64',
    'cargo_total_tarde': 'float64',
    'minutos_totales_noche': 'float64',
    'llamadas_totales_noche': 'int64',
    'cargo_total_noche': 'float64',
    'minutos_totales_internacionales': 'float64',
    'llamadas_totales_internacionales': 'int64',
    'cargo_total_internacional': 'float64',
    'num_llamadas_servicio_cliente': 'int64',

})

# Se observa los cambios en las variables de tipo de texto y se les dios el formato
datos.head()

# Selecciona las columnas numéricas del DataFrame 'datos'
datos_numericos = datos.select_dtypes(include=['float64', 'int64']).columns

# Crea una instancia de ColumnTransformer para estandarizar las columnas numéricas
estandarizacion = ColumnTransformer([('Estandarizados', StandardScaler(), datos_numericos)])

# Aplica la transformación al conjunto de datos 'datos' y almacena el resultado en 'datos_estandarizados'
datos_estandarizados = estandarizacion.fit_transform(datos[datos_numericos])

# Crea un DataFrame con las columnas numéricas estandarizadas y utiliza los nombres de columnas originales
datos_estandarizados_df = pd.DataFrame(datos_estandarizados, columns=datos_numericos)

print("Estadísticas Descriptivas de los Datos Estandarizados:")
print(datos_estandarizados_df.describe())# imprime los resultados

# Crea un gráfico de barras con los datos estandarizados
datos_estandarizados_df.plot(kind="bar", figsize=(20, 6))

# Agrega una leyenda al gráfico
plt.legend(loc=(1.01, 0.5))

# Agrega un título al gráfico
plt.title("Comparando individuos (Datos Estandarizados)")

# Muestra el gráfico
plt.show()

fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20, 8))
# Densidad de los datos originales
sns.kdeplot(data=datos[datos_numericos], fill=True, ax=axes[0])
axes[0].set_title('Distribución de Datos Original')

# Densidad de los datos estandarizados
sns.kdeplot(data=datos_estandarizados_df, fill=True, ax=axes[1])
axes[1].set_title('Distribución de Datos Estandarizados')

plt.show()

# Calcular la matriz de correlación
matriz_correlacion = datos.corr()

# Visualizar la matriz de correlación con un mapa de calor
plt.figure(figsize=(12, 10))
sns.heatmap(matriz_correlacion, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlación')
plt.show()

"""ACP"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Seleccionar las columnas para ACP
columnas_acp = ['duracion_cuenta', 'num_mensajes_correo', 'minutos_totales_dia', 'llamadas_totales_dia',
                'cargo_total_dia', 'minutos_totales_tarde', 'llamadas_totales_tarde', 'cargo_total_tarde',
                'minutos_totales_noche', 'llamadas_totales_noche', 'cargo_total_noche',
                'minutos_totales_internacionales', 'llamadas_totales_internacionales', 'cargo_total_internacional',
                'num_llamadas_servicio_cliente']

# Aplicar ACP
pca = PCA()
datos_acp = pca.fit_transform(datos[columnas_acp])

# Determinar cuántos componentes principales retener
varianza_explicada = pca.explained_variance_ratio_
componentes_principales = len(varianza_explicada)

# Visualizar la varianza explicada acumulativa
plt.plot(range(1, componentes_principales + 1), np.cumsum(varianza_explicada), marker='o', linestyle='--')
plt.title('Varianza Explicada Acumulativa')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Explicada Acumulativa')
plt.show()

# Seleccionar el número de componentes principales que explican la mayor parte de la varianza
# Aquí puedes ajustar el criterio para seleccionar el número de componentes
# Por ejemplo, podrías utilizar un umbral de varianza explicada acumulativa mínima deseada.
umbral_varianza = 0.95
numero_componentes = np.argmax(np.cumsum(varianza_explicada) >= umbral_varianza) + 1

# Crear un nuevo DataFrame con los componentes principales seleccionados
columnas_componentes_principales = [f'CP{i+1}' for i in range(numero_componentes)]
datos_acp_df = pd.DataFrame(datos_acp[:, :numero_componentes], columns=columnas_componentes_principales)

# Concatenar los datos de ACP con las columnas no incluidas en ACP
datos = pd.concat([datos.drop(columns=columnas_acp), datos_acp_df], axis=1)

# Guardar el dataset resultante de ACP en un archivo CSV
datos.to_csv('churn_process.csv', index=False)